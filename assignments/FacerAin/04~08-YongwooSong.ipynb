{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1.\n",
    "우리는 붓꽃의 **꽃받침의 길이, 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비**를 통해서, **꽃의 종류**를 구분 해 볼 것입니다. **Input**으로 주어 지는 데이터는 다음과 같습니다.\n",
    "\n",
    "- Sepal Length: 꽃받침의 길이 정보이다.\n",
    "- Sepal Width: 꽃받침의 너비 정보이다.\n",
    "- Petal Length: 꽃잎의 길이 정보이다.\n",
    "- Petal Width: 꽃잎의 너비 정보이다.\n",
    "- Species: 꽃의 종류 정보이다. **setosa / versicolor / virginica** 의 3종류로 구분된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train의 shape: (100, 4)\n",
      "X_test의 shape: (50, 4)\n",
      "y_train의 shape: (100,)\n",
      "y_test의 shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"X_train의 shape:\", X_train.shape)\n",
    "print(\"X_test의 shape:\", X_test.shape)\n",
    "print(\"y_train의 shape:\", y_train.shape)\n",
    "print(\"y_test의 shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Data Normalization\n",
    "첫 번째로, 데이터를 정규화 하는 것이 중요 할 것 같습니다. 데이터를 정규화 해 보세요.\n",
    "\n",
    "(Min - Max 정규화를 이용하면 될 것 같죠? hint: ndarray.min(), ndarray.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73684211 0.36842105 0.53947368 0.15789474]\n",
      " [0.98684211 0.38157895 0.85526316 0.26315789]\n",
      " [0.72368421 0.38157895 0.57894737 0.18421053]\n",
      " [0.65789474 0.44736842 0.17105263 0.01315789]\n",
      " [1.         0.35526316 0.86842105 0.25      ]\n",
      " [0.75       0.34210526 0.52631579 0.11842105]\n",
      " [0.67105263 0.43421053 0.17105263 0.01315789]\n",
      " [0.64473684 0.44736842 0.15789474 0.02631579]\n",
      " [0.65789474 0.48684211 0.23684211 0.03947368]\n",
      " [0.64473684 0.25       0.44736842 0.11842105]]\n"
     ]
    }
   ],
   "source": [
    "X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
    "X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Data Training\n",
    "그 다음으로는 이제 데이터를 학습 시킬 시간입니다! SVM 모듈을 import 한 후, 학습을 시켜 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 (accuracy): 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=2)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "accuracy = (sum(svm.predict(X_test) == y_test) / 50) * 100\n",
    "print(f'정확도 (accuracy): {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2.\n",
    "다음은 **MNIST 데이터**에 대해 분류를 해보는 연습을 해 보겠습니다. **MNIST**는 손글씨 데이터로, **Input Data**는 [28 x 28]의 데이터로 이루어져 있습니다. 일단, 우리가 이를 학습 시키기 전에 한번 데이터를 확인 해 볼까요?\n",
    "\n",
    "이 예제는 DNN을 이용하기 때문에, **Pytorch**로 진행 하겠습니다. 실습은 **Hyperparameter**만 고치면 됩니다.\n",
    "\n",
    "**학습률 95%에 도전 해 보세요!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d5e92f250>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 부분은 절대 변경하지 마세요.\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-0. Data Load\n",
    "데이터를 불러와 보겠습니다. `transforms.Compose`를 이용하여, 데이터를 pytorch에서 사용하는 **Tensor**형으로 바꾸고, 이를 **Gaussian Distribution**으로 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 가능한 셀입니다.\n",
    "BATCH_SIZE = 64  # 60000을 사용하면, Full-Batch 학습을 진행 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't Touch!\n",
    "custom_train_transform = transforms.Compose([  \n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "custom_test_transform = transforms.Compose([\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MNIST`를 이용하여 **MNIST** 데이터를 불러 오고, 이를 transform 해 줍니다. 또한, `DataLoader`를 이용하여, 셔플을 해준 후, 미니 배치를 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(\".\", train=True, download=True, transform=custom_train_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = MNIST(\".\", train=False, download=True, transform=custom_test_transform)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 잘 들어 왔는지 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Batch index: 0 | Batch size: 64\n",
      "input batch의 모양: (64, 1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break minibatch for-loop\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, y) in enumerate(train_loader):\n",
    "    print(' | Batch index:', batch_idx, end='')\n",
    "    print(' | Batch size:', y.size()[0])\n",
    "\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "\n",
    "    x_numpy = x.numpy()\n",
    "    y_numpy = y.numpy()\n",
    "    print('input batch의 모양:', x_numpy.shape)\n",
    "    plt.imshow(x_numpy[0].reshape(28, 28))\n",
    "    plt.title(f'Label {y_numpy[0]}')\n",
    "    plt.show()\n",
    "\n",
    "    print('break minibatch for-loop')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Deep Neural Network\n",
    "아래 셀은 Deep Neural Network를 정의 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경 가능 합니다.\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_hidden_3, num_hidden_4, num_hidden_5, num_hidden_6, num_classes):\n",
    "        \"\"\"\n",
    "        num_features: input feature 갯수\n",
    "        num_hidden_1: 첫 번째 레이어의 노드 갯수\n",
    "        num_hidden_2: 두 번째 레이어의 노드 갯수\n",
    "        num_hidden_3: 세 번째 레이어의 노드 갯수\n",
    "        num_classes: 분류하고자 하는 class 갯수\n",
    "        \"\"\"\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 수정 가능!: 레이어를 쌓는 법을 아신다면, 여기서 레이어를 쌓으셔도, 혹은 지우셔도 무방 합니다. (단, nn.Linear만)\n",
    "        self.linear_1 = nn.Linear(num_features, num_hidden_1)\n",
    "        self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_3 = nn.Linear(num_hidden_2, num_hidden_3)\n",
    "        self.linear_4 = nn.Linear(num_hidden_3, num_hidden_4)\n",
    "        self.linear_5 = nn.Linear(num_hidden_4, num_hidden_5)\n",
    "        self.linear_6 = nn.Linear(num_hidden_5, num_hidden_6)\n",
    "        self.linear_out = nn.Linear(num_hidden_6, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.linear_3(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.linear_4(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.linear_5(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.linear_6(out)\n",
    "        out = torch.relu(out)\n",
    "        logits = self.linear_out(out)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return logits, probas\n",
    "\n",
    "# 수정 가능!\n",
    "model = DNN(num_features=28*28, #784\n",
    "        num_hidden_1=512,\n",
    "        num_hidden_2=256,\n",
    "        num_hidden_3=128,\n",
    "        num_hidden_4=64,\n",
    "        num_hidden_5=32,\n",
    "        num_hidden_6=16,\n",
    "        num_classes=10)\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Training\n",
    "여기서는 **Optimizer**와, **Epoch**를 설정 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 가능!\n",
    "LEARNING_LATE = 0.01  # 흠.. 이 친구는 학습률로는 너무 큰 것 같네요..\n",
    "NUM_EPOCHS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_LATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/007 | Batch 000/937 | Cost: 2.3368\n",
      "Epoch: 001/007 | Batch 040/937 | Cost: 2.3085\n",
      "Epoch: 001/007 | Batch 080/937 | Cost: 2.3225\n",
      "Epoch: 001/007 | Batch 120/937 | Cost: 2.2968\n",
      "Epoch: 001/007 | Batch 160/937 | Cost: 2.2938\n",
      "Epoch: 001/007 | Batch 200/937 | Cost: 2.3140\n",
      "Epoch: 001/007 | Batch 240/937 | Cost: 2.3317\n",
      "Epoch: 001/007 | Batch 280/937 | Cost: 2.3105\n",
      "Epoch: 001/007 | Batch 320/937 | Cost: 2.3046\n",
      "Epoch: 001/007 | Batch 360/937 | Cost: 2.2750\n",
      "Epoch: 001/007 | Batch 400/937 | Cost: 2.2855\n",
      "Epoch: 001/007 | Batch 440/937 | Cost: 2.3075\n",
      "Epoch: 001/007 | Batch 480/937 | Cost: 2.2950\n",
      "Epoch: 001/007 | Batch 520/937 | Cost: 2.3081\n",
      "Epoch: 001/007 | Batch 560/937 | Cost: 2.3102\n",
      "Epoch: 001/007 | Batch 600/937 | Cost: 2.2987\n",
      "Epoch: 001/007 | Batch 640/937 | Cost: 2.3040\n",
      "Epoch: 001/007 | Batch 680/937 | Cost: 2.3006\n",
      "Epoch: 001/007 | Batch 720/937 | Cost: 2.2958\n",
      "Epoch: 001/007 | Batch 760/937 | Cost: 2.3030\n",
      "Epoch: 001/007 | Batch 800/937 | Cost: 2.3057\n",
      "Epoch: 001/007 | Batch 840/937 | Cost: 2.3101\n",
      "Epoch: 001/007 | Batch 880/937 | Cost: 2.2995\n",
      "Epoch: 001/007 | Batch 920/937 | Cost: 2.2957\n",
      "Epoch: 001/007 Train Acc.: 10.44% | Test Acc.: 10.28%\n",
      "Time elapsed: 4.17 min\n",
      "Epoch: 002/007 | Batch 000/937 | Cost: 2.2992\n",
      "Epoch: 002/007 | Batch 040/937 | Cost: 2.2929\n",
      "Epoch: 002/007 | Batch 080/937 | Cost: 2.3003\n",
      "Epoch: 002/007 | Batch 120/937 | Cost: 2.3082\n",
      "Epoch: 002/007 | Batch 160/937 | Cost: 2.3001\n",
      "Epoch: 002/007 | Batch 200/937 | Cost: 2.3024\n",
      "Epoch: 002/007 | Batch 240/937 | Cost: 2.2887\n",
      "Epoch: 002/007 | Batch 280/937 | Cost: 2.2871\n",
      "Epoch: 002/007 | Batch 320/937 | Cost: 2.2981\n",
      "Epoch: 002/007 | Batch 360/937 | Cost: 2.3063\n",
      "Epoch: 002/007 | Batch 400/937 | Cost: 2.2991\n",
      "Epoch: 002/007 | Batch 440/937 | Cost: 2.2870\n",
      "Epoch: 002/007 | Batch 480/937 | Cost: 2.2954\n",
      "Epoch: 002/007 | Batch 520/937 | Cost: 2.2998\n",
      "Epoch: 002/007 | Batch 560/937 | Cost: 2.2789\n",
      "Epoch: 002/007 | Batch 600/937 | Cost: 2.2939\n",
      "Epoch: 002/007 | Batch 640/937 | Cost: 2.2892\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy_and_loss(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    cross_entropy = 0.\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.view(-1, 28*28).to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        cross_entropy += F.cross_entropy(logits, targets).item()\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100, cross_entropy/num_examples\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "train_acc_lst, test_acc_lst = [], []\n",
    "train_loss_lst, test_loss_lst = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "    \n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.view(-1, 28*28).to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)  # 모델 계산\n",
    "        cost = F.cross_entropy(logits, targets)  # 크로스 엔트로피 계산\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        \n",
    "        cost.backward()  # 역전파\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()  # step 적용\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 40:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "\n",
    "    # 매 Epoch마다 evaluation을 진행합니다. \n",
    "    # Epoch마다 Loss를 기록하여 학습과정을 살펴보고 Underfitting, Overfitting 여부를 확인합니다.\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # Gradient 계산이 안되도록\n",
    "        train_acc, train_loss = compute_accuracy_and_loss(model, train_loader, device=DEVICE) # train acc, loss 계산\n",
    "        test_acc, test_loss = compute_accuracy_and_loss(model, test_loader, device=DEVICE)    # test acc, loss 계산\n",
    "        \n",
    "        # list에 train, test의  acc, loss 추가\n",
    "        train_acc_lst.append(train_acc)\n",
    "        test_acc_lst.append(test_acc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "        test_loss_lst.append(test_loss)\n",
    "        \n",
    "        # 로깅\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Test Acc.: {test_acc:.2f}%')\n",
    "    \n",
    "    # 1 epoch 학습 소요시간\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "\n",
    "# 총 학습 소요시간\n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. Evaluation\n",
    "테스트 데이터와 학습 데이터의 Loss 변화를 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), train_loss_lst, label='Training loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_loss_lst, label='Test loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), train_acc_lst, label='Training accuracy')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_acc_lst, label='Test accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss = compute_accuracy_and_loss(model, test_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "\n",
    "    x_numpy = x.numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5)\n",
    "    \n",
    "    for ax in axes:\n",
    "        index = random.randint(0, 64)\n",
    "        logits, probas = model(x[index].view(-1, 28*28))\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        ax.imshow(x_numpy[index].reshape(28, 28))\n",
    "        ax.set_title(f'Label {predicted_labels[0]}')\n",
    "        \n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. Discussion\n",
    "1. Train Data에 대한 정확도와, Test Data에 대한 정확도가 왜 다를까요?\n",
    "- 입력 해 주세요!\n",
    "\n",
    "2. 다른 사람들은 정확도가 99퍼가 넘는 모델도 만들던데, DNN의 한계가 있다면 어떤 점이 있을까요? (Hint: 우리는 28x28의 이미지를 768x1로 쫙 펴서 넣어 줬습니다.)\n",
    "- 입력 해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
